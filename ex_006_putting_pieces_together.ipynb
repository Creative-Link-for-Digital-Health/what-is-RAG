{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51adc40",
   "metadata": {},
   "source": [
    "# Putting all of the Pieces Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ba1b8",
   "metadata": {},
   "source": [
    "Now that we know how to evaluate semantic similarity using a database, we can proceed to an actual RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install chromadb\n",
    "! pip install ollama\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85a68c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import uuid\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "from openai import OpenAI\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from api_utils import load_api_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer models that will help us split our text into sentences.\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f60e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./heart_attack.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b39ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break our story up into seperate sentances\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "sentences = tokenizer.tokenize(text)\n",
    "\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(sentences[33])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8272e63a",
   "metadata": {},
   "source": [
    "## Now that our sentences are split, we can generate embeddings for individual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc9b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Nomic model served locally via Ollama for embedding\n",
    "# Ollama is a friend --> https://ollama.com/\n",
    "def get_embeddings_from_ollama(text, model=\"nomic-embed-text\"):\n",
    "    url = \"http://localhost:11434/api/embeddings\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": text\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    return np.array(response.json()[\"embedding\"], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212888d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for sentence in sentences:\n",
    "    embedding = get_embeddings_from_ollama(sentence)\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73cc78",
   "metadata": {},
   "source": [
    "## And now, on to packing everything into a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54e65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac4aa569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique collection and add the embeddings to it\n",
    "\n",
    "unique_collection_name = f\"document_sentences_{int(time.time())}\"\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=unique_collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Using cosine similarity\n",
    ")\n",
    "\n",
    "# Generate IDs for each sentence\n",
    "ids = [str(uuid.uuid4()) for _ in embeddings]\n",
    "\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    embeddings=embeddings,\n",
    "    documents=sentences\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95935a",
   "metadata": {},
   "source": [
    "## And now ... bringing in the LLM and the full RAG experience with semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4053671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API parameters and initialize client\n",
    "\n",
    "SECRETS_PATH = \".secrets.toml\"\n",
    "\n",
    "API_CALL_PARAMS = load_api_params(SECRETS_PATH)\n",
    "client = OpenAI(\n",
    "    base_url = API_CALL_PARAMS['API_URL'],\n",
    "    api_key = API_CALL_PARAMS['API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6536c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(model: str, messages: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Generate LLM output\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "290a706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = \"\"\"Can you please tell me what is a heart attack.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c68c31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = get_embeddings_from_ollama(USER_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "724aed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e966d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=[query_vector],\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "print(\"Query results for:\", USER_PROMPT)\n",
    "for i, (doc, distance) in enumerate(zip(results[\"documents\"][0], results[\"distances\"][0])):\n",
    "    print(f\"Result {i+1}: {doc}\")\n",
    "    KNOWLEDGE += str(doc)+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ed3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"Answer all user questions to the best of your ability. Use the following text for reference:\n",
    "\n",
    "{KNOWLEDGE}\n",
    "\"\"\"\n",
    "\n",
    "print(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa2daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"\"\"{SYSTEM_PROMPT}\"\"\"},\n",
    "    {\"role\": \"user\", \"content\":f\"\"\"{USER_PROMPT}\"\"\"}\n",
    "]\n",
    "try:\n",
    "    model = API_CALL_PARAMS['MODEL']\n",
    "    LLM_output = generate_completion(model, messages)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error generating completion: {e}\")\n",
    "\n",
    "print(LLM_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-presentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
